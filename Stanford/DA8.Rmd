---
title: "DA8"
output: html_document
date: "2025-06-29"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

9.6.1 Support Vector Classifier
```{r}
set.seed(1)
x=matrix(rnorm (20*2), ncol=2)
y=c(rep(-1,10), rep(1,10))
x[y==1,]=x[y==1,] + 1
plot(x, col=(3-y))
```
We generated the observations to check whether they are linearly separable. They are not. 

```{r}
dat=data.frame(x=x, y=as.factor(y))
library(e1071)
svmfit=svm(y~., data=dat , kernel ="linear", cost=10, scale=FALSE)
```
The svm won't scale each feature to have mean zero or standard deviation one because scale = FALSE

```{r}
plot(svmfit , dat)
```
```{r}
svmfit$index
summary(svmfit)
```
This tells us, for instance, that a linear kernel was used with cost=10, and that there were seven support vectors, four in one class and three in the other.

What if we use a smaller value of cost parameter? 

```{r}
svmfit=svm(y~., data=dat , kernel ="linear", cost =0.1,
  scale=FALSE)
plot(svmfit , dat)
```
```{r}
svmfit$index

```

we have a larger number of support vectors, because the margin is now wider

```{r}
set.seed(1)
tune.out=tune(svm ,y~.,data=dat ,kernel ="linear",
  ranges=list(cost=c (0.001, 0.01, 0.1, 1,5,10,100) ))
summary(tune.out)
```
0.05 error is for 0.1 cost, so it is the best one. Tune() stores the best function and we can assess: 

```{r}
best.mod = tune.out$best.model
summary(best.mod)
```
```{r}
xtest=matrix(rnorm (20*2) , ncol=2)
ytest=sample (c(-1,1), 20, rep=TRUE)
xtest[ytest==1,]= xtest[ytest==1,] + 1
testdat=data.frame(x= xtest , y=as.factor(ytest))
```
now we made the test set 

```{r}
ypred=predict(best.mod ,testdat)
table(predict = ypred , truth=testdat$y)
mean(ypred == testdat$y)
```
We see that the model predicted with 85% accuracy 

Now let's imagine that the observations are barely linearly separable. We fit the support vector classifier and plot the resulting hyperplane, using a very large value of cost so that no observations are misclassified.

```{r}
dat=data.frame(x=x,y=as.factor(y))
svmfit=svm(y~., data=dat , kernel ="linear", cost=1e5)
summary(svmfit)
```
```{r}
x[y==1,]=x[y==1,]+0.5
plot(x, col=(y+5)/2, pch =19)

```

No training errors were made and only three support vectors were used, however we can see that the margin is very narrow which might lead to the errors with test data. 

```{r}
dat=data.frame(x=x,y=as.factor(y))
svmfit=svm(y~., data=dat , kernel ="linear", cost=1e5)
summary(svmfit)
```


```{r}
xtest=matrix(rnorm (20*2) , ncol=2)
ytest=sample (c(-1,1), 20, rep=TRUE)
xtest[ytest==1,]= xtest[ytest==1,] + 1
testdat=data.frame(x= xtest , y=as.factor(ytest))
ypred=predict(svmfit ,testdat)
table(predict = ypred , truth=testdat$y)
mean(ypred == testdat$y)
```
it is only 65% accuracy 


```{r}
svmfit=svm(y~., data=dat , kernel ="linear", cost=1)
summary(svmfit)
```

```{r}
plot(svmfit ,dat)
```

Using cost=1, we misclassify a training observation, but we also obtain a much wider margin and use seven support vectors. This will eventually perform better on test data. 


```{r}
xtest=matrix(rnorm (20*2) , ncol=2)
ytest=sample (c(-1,1), 20, rep=TRUE)
xtest[ytest==1,]= xtest[ytest==1,] + 1
testdat=data.frame(x= xtest , y=as.factor(ytest))
ypred=predict(svmfit ,testdat)
table(predict = ypred , truth=testdat$y)
mean(ypred == testdat$y)
```
This has a 80% accuracy. 



9.6.2 Support Vector Machine
```{r}
set.seed(1)
x=matrix(rnorm (200*2) , ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150 ,]=x[101:150,]-2
y=c(rep(1,150) ,rep(2,50))
dat=data.frame(x=x,y=as.factor(y))
```

we created the non-linear class boundary 

```{r}
plot(x, col=y)
```
This the plot of data

```{r}
train=sample (200,100)
svmfit=svm(y~., data=dat[train ,], kernel ="radial", gamma=1,
  cost=1)
plot(svmfit , dat[train ,])
```
We randomly divided the data into 2 parts and plotted it. There are some mistakes in training set. 

```{r}
summary(svmfit)
```
Maybe we can decrease the errors by increasing the cost, however it might be overfitting. 
```{r}
svmfit=svm(y~., data=dat[train ,], kernel ="radial", gamma=1,
  cost=1e5)
plot(svmfit ,dat[train ,])
```
Let's now check the performance 

```{r}
set.seed(1)
tune.out=tune(svm , y~., data=dat[train ,], kernel ="radial",
  ranges=list(cost=c(0.1,1,10,100,1000),
  gamma=c(0.5,1,2,3,4) ))
summary (tune.out)
```
The best is when cost is 1 and gamma is 1

```{r}
table(true=dat[-train ,"y"], pred=predict(tune.out$best.model,
  newdata =dat[-train ,]))
mean(predict(tune.out$best.model, newdata =dat[-train ,]) == dat[-train ,"y"])
```
there is 88% accuracy. 

9.6.3 ROC Curves
```{r}
library(ROCR)
rocplot = function(pred , truth , ...){
  predob = prediction(pred, truth)
  perf = performance(predob , "tpr", "fpr")
  plot(perf, ...)}
```

A short function to plot an ROC curve given a vector containing a numerical score for each observation, pred, and a vector containing the class label for each observation, truth.

```{r}
svmfit.opt=svm(y~., data=dat[train ,], kernel ="radial",
  gamma=7, cost=1, decision.values = T)
fitted =attributes(predict(svmfit.opt ,dat[ train ,], 
                           decision.values=TRUE))$decision.values
```
Here we obtained fitted values 

```{r}
par(mfrow=c(1,2))
rocplot(fitted, dat[train, "y"], main="Training Data")
```
This curve is not as good, but it is for training data. 

```{r}
svmfit.flex=svm(y~., data=dat[train ,], kernel ="radial",
  gamma=50, cost=1, decision.values =T)
fitted=attributes (predict (svmfit.flex ,dat[ train ,], 
                            decision.values=T))$decision.values
rocplot(fitted, dat[train ,"y"], col="red")
```
```{r}
fitted =attributes (predict (svmfit.opt ,dat[- train ,], 
                             decision.values=T))$decision.values
rocplot (fitted ,dat[-train ,"y"], main="Test Data")
fitted=attributes (predict (svmfit.flex ,dat[- train ,], 
                            decision.values=T))$decision.values
rocplot (fitted ,dat[-train ,"y"],add=T,col="red")
```
Although the curve is still pretty bad, it is better on test data set. 
