---
title: "DA3"
output: html_document
date: "2025-06-20"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

10) 
```{r}
carseats = read.csv("~/Downloads/Carseats.csv")
contrasts(factor(carseats$Urban))
contrasts(factor(carseats$US))
summary(lm(carseats$Sales ~ carseats$Price + carseats$Urban + carseats$US))
```
b) 
According to these, if there was no effect of different predictors (Price was 0, Urban was No and US was No), the sales would be estimately 13 thousands. We can trust this because t value > 2 and Pr(>|t|) is too small for the estimate to change a lot
Price has a statistically significant effect on the sales too. Every increase by 1 dollar in price leades to reduction in total sales on 0.054. We can trust this because t value > 2 and Pr(>|t|) is too small for the Price to change a lot
Urban is not statistically significant, it probably has no effect on the sales because t value < 2 and Pr(>|t|) is quite big, so it might vary
US is statistically significant because t value > 2 and Pr(>|t|) is too small for the estimate to change a lot. As it is linear regression, it is not really suitable for categorical variables, but the location in US improves sales by 1.2

c) 
Sales = A + B1*Price + B2*Urban + B3*US
Sales =13.0435+(−0.05446)Price +(−0.02192)Urban +1.20057US 
If Urban and US are 1 (those dummy variables R created), those are substracted/added 

d) the Null Hypothesis is that there is no relationship between this predictor and the y. In order to reject/fail to reject those we need to do:
```{r}
contrasts(factor(carseats$ShelveLoc))
summary(lm(carseats$Sales ~ ., data = carseats))
```
so from the results we have, we can make several conclusions: 
those with a large t value (>2) are statistically significant, therefore for them we can reject the null hypothesis. 
Predictors such as CompPrice, Income, Advertising, Price, ShelveLocGood, ShelveLocMedium and Age actully influence the overall Sales. 

For other predictors (Population, Education, UrbanYes, USYes) we fail to reject the null hypothesis and we don't whether they do significantly affect the sales. 

e) 
```{r}
summary(lm(carseats$Sales ~ carseats$CompPrice + carseats$Income + carseats$Advertising + carseats$Price + carseats$ShelveLoc + carseats$Age))
```
f) If we compare the Multiple R-squared and Adjusted R-squared from a and from e: 
In a only 23.35-23.93 % of sales variance is explained by those predictors, but in e 86.97-87.2 % of sales variance is explained by predictors. The E model fits data better. 

g) 
```{r}
confint(lm(carseats$Sales ~ carseats$CompPrice + carseats$Income + carseats$Advertising + carseats$Price + carseats$ShelveLoc + carseats$Age), level = 0.95)
```
we can calculate those using formula a +- t value * std, but there is a built-in function in r for this. From this we can say that if there's no influence of predictors (each continuous is equal to 0 and categorical are no/bad), the sales in 95% of cases will be something between 4.48 and 6.47. 
Also, in 95% of cases: 
One increase at CompPrice will increase the sales by 0.084-0.1
One dollar increase at Income will increase the sales by 0.012 - 0.019
One dollar increase at Advertising will increase the sales by 0.1-0.13  
One dollar increase at Price will decrease the sales by 0.09-0.1
Good ShelveLoc will increase the sales by 4.536 - 5.135
Medium ShelveLoc will increase the sales by 1.706-2.198 
One increase of age will decrease the sales by 0.039-0.052

g) 

```{r}
par(mfrow = c(2, 2))

plot(lm(carseats$Sales ~ carseats$CompPrice + carseats$Income + carseats$Advertising + carseats$Price + carseats$ShelveLoc + carseats$Age))

par(mfrow = c(1,1))
```
```{r}
model_e   <- lm(Sales ~ CompPrice + Income + Advertising + Price + ShelveLoc + Age,
                data = carseats)

r_stud  <- rstudent(model_e)
h       <- hatvalues(model_e)

n <- nrow(carseats)
p <- length(coef(model_e)) - 1

outl_cut  <- 2            # |r_stud| > 2
lev_cut   <- 2*(p+1)/n    # h > 2(p+1)/n

flags <- which(
  abs(r_stud) > outl_cut |
  h           > lev_cut  
)

flags
data.frame(
  Obs       = flags,
  Rstud     = r_stud[flags],
  Leverage  = h[flags]
)

```
from this we can see that there are some outliers and hatvalues.

R LAB
3.6.1
```{r}
library(MASS)
library(ISLR)
```
3.6.2 
```{r}
names(Boston)

attach(Boston)
model = lm(medv~lstat)
summary(model)
```


```{r}
coef(model)
```


```{r}
confint(model)
```


```{r}
predict(model, data.frame(lstat=c(5, 10 ,15)),
        interval ="confidence")
```
by default leve is 95%; The "predict" returns the three columns; 
fit: the point estimate y
lwr: lower bound of the 95% for the mean lstat.
upr: upper bound of the 95%.

```{r}
predict(model, data.frame(lstat=c(5, 10 ,15)),
        interval ="prediction")
```
by default level is 95%; The "predict" returns the three columns; 
fit: the point estimate y
lwr: lower bound of the 95% for the mean lstat.
upr: upper bound of the 95%.

So the mean at both "confidence" and at "prediction" is the same (~25), however range is different. 24.47 - 25.63 at confidence and 12.83-37.28 at prediction 

```{r}
plot(lstat ,medv ,pch = 20)
abline(model, col ="red")
```

```{r}
par(mfrow = c(2,2))
plot(model)
```

```{r}
plot(predict (model), residuals (model))
```
Residuals tend to be positive for smaller fitted values and negative for larger ones, so it is another evidence of non-linearity.

More points on the right

A few extreme points (>15) appear as outliers.

```{r}
plot(hatvalues (model))
```
points around 400 are extreme hat values which can influence the overall regression pattern. 

```{r}
which.max(hatvalues (model))
```

3.6.3 Multiple Linear Regression
```{r}
 lm.fit = lm(medv ~ lstat+age, data = Boston) 
summary(lm.fit)
```
```{r}
lm.fit=lm(medv~.,data=Boston)
summary (lm.fit)

summary(lm.fit)$r.sq

summary(lm.fit)$sigma

library(car)
vif(lm.fit)

lm.fit1=lm(medv~.-age ,data=Boston )
summary (lm.fit1)
```
3.6.4 Interaction Terms
```{r}
summary (lm(medv~lstat*age ,data=Boston))
```

we see that this interaction is actually not helping, but we can find better interactions.
```{r}
summary (lm(medv~lstat*rm ,data=Boston))
```

